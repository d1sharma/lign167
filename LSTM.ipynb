{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('imdb_master.csv',encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0', 'type','file'], axis=1)\n",
    "df = df[df.label != 'unsup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49940</th>\n",
       "      <td>Oh yeah! Jenna Jameson did it again! Yeah Baby...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49941</th>\n",
       "      <td>This one is quite a nice surprise. Cute little...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49942</th>\n",
       "      <td>I watched this movie once and might watch it a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49943</th>\n",
       "      <td>Very nice action with an interwoven story whic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49944</th>\n",
       "      <td>If you've ever wondered why they don't make po...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49945</th>\n",
       "      <td>Before watching this movie I thought this movi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49946</th>\n",
       "      <td>this is really films outside (not in a motel r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49947</th>\n",
       "      <td>This is a more interesting than usual porn mov...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49948</th>\n",
       "      <td>Saw this movie twice at community screenings a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49949</th>\n",
       "      <td>Beforehand Notification: I'm sure someone is g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49950</th>\n",
       "      <td>This film is shockingly underrated on IMDb. Li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49951</th>\n",
       "      <td>I watch them all.&lt;br /&gt;&lt;br /&gt;It's not better t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49952</th>\n",
       "      <td>Well, if you are open-minded enough to have li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49953</th>\n",
       "      <td>This movie surprised me. Some things were \"cli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49954</th>\n",
       "      <td>I noticed this movie was getting trashed well ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49955</th>\n",
       "      <td>I must give How She Move a near-perfect rating...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49956</th>\n",
       "      <td>I was a bit scared to watch this movie due to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49957</th>\n",
       "      <td>I saw this movie when I was a little girl. And...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49958</th>\n",
       "      <td>Just get it. The DVD is cheap and easy to come...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49959</th>\n",
       "      <td>Perfect for families with small children who a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49960</th>\n",
       "      <td>I saw this movie when Mystery Science Theater ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49961</th>\n",
       "      <td>There are some things I will never understand;...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49962</th>\n",
       "      <td>My mother took me to see this film as a child ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49963</th>\n",
       "      <td>Shawshank, Godfather, Pulp Fiction... all good...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49964</th>\n",
       "      <td>This movie surprised me. Some things were \"cli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49965</th>\n",
       "      <td>Brilliant use of overstated technicolor illust...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49966</th>\n",
       "      <td>This film was Excellent, I thought that the or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49967</th>\n",
       "      <td>This is definitely an appropriate update for t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49968</th>\n",
       "      <td>I really liked this version of 'Vanishing Poin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49969</th>\n",
       "      <td>Sure this was a remake of a 70's film, but it ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49970</th>\n",
       "      <td>its not as good as the first movie,but its a g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49971</th>\n",
       "      <td>Sure, it was cheesy and nonsensical and at tim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49972</th>\n",
       "      <td>SPOILERS THROUGH: &lt;br /&gt;&lt;br /&gt;I really am in t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49973</th>\n",
       "      <td>I have to say, I loved Vanishing Point. I've s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49974</th>\n",
       "      <td>To start off with, since this movie is a remak...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49975</th>\n",
       "      <td>I have to agree with most of the other posts. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49976</th>\n",
       "      <td>Any movie that shows federal PIGs (Persons In ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49977</th>\n",
       "      <td>In Canadian director Kari Skogland's film adap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49978</th>\n",
       "      <td>I saw this movie last night after waiting ages...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49979</th>\n",
       "      <td>This movie is about basically human relations,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49980</th>\n",
       "      <td>I was surprised at just how much I enjoyed thi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49981</th>\n",
       "      <td>I saw this film in Winnipeg recently - appropr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49982</th>\n",
       "      <td>As perhaps one of the few Canadians who did no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49983</th>\n",
       "      <td>I was very moved by the story and because I am...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49984</th>\n",
       "      <td>What I loved about the on-screen adaptation of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49985</th>\n",
       "      <td>I had a chance to see a screening of this movi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49986</th>\n",
       "      <td>This is a really interesting movie. It is an a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49987</th>\n",
       "      <td>I saw the movie recently and really liked it. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49988</th>\n",
       "      <td>I thought this movie was hysterical. I have wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49989</th>\n",
       "      <td>...this is a classic with so many great dialog...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49990</th>\n",
       "      <td>The most hillarious and funny Brooks movie I e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49991</th>\n",
       "      <td>\"Life stinks\" is a parody of life and death, h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49992</th>\n",
       "      <td>This is the kind of film you want to see with ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49993</th>\n",
       "      <td>I have not read the other comments on the film...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49994</th>\n",
       "      <td>Life Stinks (1991) was a step below Mel Brooks...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>Seeing as the vote average was pretty low, and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The plot had some wretched, unbelievable twist...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am amazed at how this movie(and most others ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>A Christmas Together actually came before my t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>Working-class romantic drama from director Mar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  label\n",
       "49940  Oh yeah! Jenna Jameson did it again! Yeah Baby...      1\n",
       "49941  This one is quite a nice surprise. Cute little...      1\n",
       "49942  I watched this movie once and might watch it a...      1\n",
       "49943  Very nice action with an interwoven story whic...      1\n",
       "49944  If you've ever wondered why they don't make po...      1\n",
       "49945  Before watching this movie I thought this movi...      1\n",
       "49946  this is really films outside (not in a motel r...      1\n",
       "49947  This is a more interesting than usual porn mov...      1\n",
       "49948  Saw this movie twice at community screenings a...      1\n",
       "49949  Beforehand Notification: I'm sure someone is g...      1\n",
       "49950  This film is shockingly underrated on IMDb. Li...      1\n",
       "49951  I watch them all.<br /><br />It's not better t...      1\n",
       "49952  Well, if you are open-minded enough to have li...      1\n",
       "49953  This movie surprised me. Some things were \"cli...      1\n",
       "49954  I noticed this movie was getting trashed well ...      1\n",
       "49955  I must give How She Move a near-perfect rating...      1\n",
       "49956  I was a bit scared to watch this movie due to ...      1\n",
       "49957  I saw this movie when I was a little girl. And...      1\n",
       "49958  Just get it. The DVD is cheap and easy to come...      1\n",
       "49959  Perfect for families with small children who a...      1\n",
       "49960  I saw this movie when Mystery Science Theater ...      1\n",
       "49961  There are some things I will never understand;...      1\n",
       "49962  My mother took me to see this film as a child ...      1\n",
       "49963  Shawshank, Godfather, Pulp Fiction... all good...      1\n",
       "49964  This movie surprised me. Some things were \"cli...      1\n",
       "49965  Brilliant use of overstated technicolor illust...      1\n",
       "49966  This film was Excellent, I thought that the or...      1\n",
       "49967  This is definitely an appropriate update for t...      1\n",
       "49968  I really liked this version of 'Vanishing Poin...      1\n",
       "49969  Sure this was a remake of a 70's film, but it ...      1\n",
       "49970  its not as good as the first movie,but its a g...      1\n",
       "49971  Sure, it was cheesy and nonsensical and at tim...      1\n",
       "49972  SPOILERS THROUGH: <br /><br />I really am in t...      1\n",
       "49973  I have to say, I loved Vanishing Point. I've s...      1\n",
       "49974  To start off with, since this movie is a remak...      1\n",
       "49975  I have to agree with most of the other posts. ...      1\n",
       "49976  Any movie that shows federal PIGs (Persons In ...      1\n",
       "49977  In Canadian director Kari Skogland's film adap...      1\n",
       "49978  I saw this movie last night after waiting ages...      1\n",
       "49979  This movie is about basically human relations,...      1\n",
       "49980  I was surprised at just how much I enjoyed thi...      1\n",
       "49981  I saw this film in Winnipeg recently - appropr...      1\n",
       "49982  As perhaps one of the few Canadians who did no...      1\n",
       "49983  I was very moved by the story and because I am...      1\n",
       "49984  What I loved about the on-screen adaptation of...      1\n",
       "49985  I had a chance to see a screening of this movi...      1\n",
       "49986  This is a really interesting movie. It is an a...      1\n",
       "49987  I saw the movie recently and really liked it. ...      1\n",
       "49988  I thought this movie was hysterical. I have wa...      1\n",
       "49989  ...this is a classic with so many great dialog...      1\n",
       "49990  The most hillarious and funny Brooks movie I e...      1\n",
       "49991  \"Life stinks\" is a parody of life and death, h...      1\n",
       "49992  This is the kind of film you want to see with ...      1\n",
       "49993  I have not read the other comments on the film...      1\n",
       "49994  Life Stinks (1991) was a step below Mel Brooks...      1\n",
       "49995  Seeing as the vote average was pretty low, and...      1\n",
       "49996  The plot had some wretched, unbelievable twist...      1\n",
       "49997  I am amazed at how this movie(and most others ...      1\n",
       "49998  A Christmas Together actually came before my t...      1\n",
       "49999  Working-class romantic drama from director Mar...      1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df['label'].str.lower().replace({'pos': 1, 'neg': 0})\n",
    "df.tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMSentiment(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size, use_gpu, batch_size, dropout=0.5):\n",
    "        super(LSTMSentiment, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_gpu = use_gpu\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        if self.use_gpu:\n",
    "            return (Variable(torch.zeros(1, self.batch_size, self.hidden_dim).cuda()),\n",
    "                    Variable(torch.zeros(1, self.batch_size, self.hidden_dim).cuda()))\n",
    "        else:\n",
    "            return (Variable(torch.zeros(1, self.batch_size, self.hidden_dim)),\n",
    "                    Variable(torch.zeros(1, self.batch_size, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        x = self.embeddings(sentence).view(len(sentence), self.batch_size, -1)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y = self.hidden2label(lstm_out[-1])\n",
    "        log_probs = F.log_softmax(y,dim=0)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load word embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train epoch 1:   0%|          | 0/6001 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Writing to /Users/ribhu/Desktop/Curr_Work/LIGN167/runs/1544071017\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:73: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "\n",
      "Train epoch 1:   0%|          | 1/6001 [00:00<1:20:19,  1.25it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 2/6001 [00:01<1:03:37,  1.57it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 3/6001 [00:01<51:38,  1.94it/s]  \u001b[A\n",
      "Train epoch 1:   0%|          | 4/6001 [00:01<44:07,  2.27it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 5/6001 [00:01<38:36,  2.59it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 6/6001 [00:02<34:36,  2.89it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 7/6001 [00:02<34:07,  2.93it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 8/6001 [00:02<32:14,  3.10it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 9/6001 [00:02<30:29,  3.28it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 10/6001 [00:03<29:41,  3.36it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 11/6001 [00:03<28:49,  3.46it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 12/6001 [00:03<29:13,  3.42it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 13/6001 [00:04<29:00,  3.44it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 14/6001 [00:04<28:31,  3.50it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 15/6001 [00:04<28:02,  3.56it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 16/6001 [00:04<27:54,  3.58it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 17/6001 [00:05<27:32,  3.62it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 18/6001 [00:05<27:46,  3.59it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 19/6001 [00:05<27:45,  3.59it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 20/6001 [00:06<28:07,  3.54it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 21/6001 [00:06<28:06,  3.55it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 22/6001 [00:07<42:32,  2.34it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 23/6001 [00:07<52:48,  1.89it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 24/6001 [00:08<51:57,  1.92it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 25/6001 [00:08<47:40,  2.09it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 26/6001 [00:09<45:09,  2.21it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 27/6001 [00:09<50:36,  1.97it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 28/6001 [00:10<44:29,  2.24it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 29/6001 [00:10<45:40,  2.18it/s]\u001b[A\n",
      "Train epoch 1:   0%|          | 30/6001 [00:11<48:55,  2.03it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 31/6001 [00:11<45:59,  2.16it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 32/6001 [00:11<42:16,  2.35it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 33/6001 [00:12<39:37,  2.51it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 34/6001 [00:12<39:47,  2.50it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 35/6001 [00:12<36:32,  2.72it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 36/6001 [00:13<34:40,  2.87it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 37/6001 [00:13<33:59,  2.92it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 38/6001 [00:13<32:22,  3.07it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 39/6001 [00:14<32:00,  3.10it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 40/6001 [00:14<31:13,  3.18it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 41/6001 [00:14<31:01,  3.20it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 42/6001 [00:15<31:36,  3.14it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 43/6001 [00:15<31:45,  3.13it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 44/6001 [00:15<32:00,  3.10it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 45/6001 [00:16<32:54,  3.02it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 46/6001 [00:16<37:51,  2.62it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 47/6001 [00:16<39:53,  2.49it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 48/6001 [00:17<39:55,  2.48it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 49/6001 [00:17<39:12,  2.53it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 50/6001 [00:18<38:31,  2.57it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 51/6001 [00:18<37:23,  2.65it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 52/6001 [00:18<36:28,  2.72it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 53/6001 [00:19<34:59,  2.83it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 54/6001 [00:19<34:54,  2.84it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 55/6001 [00:19<37:10,  2.67it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 56/6001 [00:20<38:56,  2.54it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 57/6001 [00:20<39:47,  2.49it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 58/6001 [00:21<39:04,  2.54it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 59/6001 [00:21<38:02,  2.60it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 60/6001 [00:21<37:21,  2.65it/s]\u001b[A\n",
      "Train epoch 1:   1%|          | 61/6001 [00:22<37:18,  2.65it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-ed921a0c334f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train: loss %.2f acc %.1f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mdev_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Dev'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-ed921a0c334f>\u001b[0m in \u001b[0;36mtrain_epoch_progress\u001b[0;34m(model, train_iter, loss_function, optimizer, text_field, label_field, epoch)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time, random\n",
    "from tqdm import tqdm\n",
    "from torchtext import data\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "torch.device('cpu')\n",
    "#For when the custom don't work\n",
    "def load_bin_vec(fname, vocab):\n",
    "    \"\"\"\n",
    "    Loads 300x1 word vecs from Google (Mikolov) word2vec\n",
    "    \"\"\"\n",
    "    word_vecs = {}\n",
    "    with open(fname, \"rb\") as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, layer1_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "        for line in range(vocab_size):\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1).decode('latin-1')\n",
    "                if ch == ' ':\n",
    "                    word = ''.join(word)\n",
    "                    break\n",
    "                if ch != '\\n':\n",
    "                    word.append(ch)\n",
    "            if word in vocab:\n",
    "               word_vecs[word] = np.frombuffer(f.read(binary_len), dtype='float32')\n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "    return word_vecs\n",
    "\n",
    "def train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.ix[perm[:train_end]]\n",
    "    validate = df.ix[perm[train_end:validate_end]]\n",
    "    test = df.ix[perm[validate_end:]]\n",
    "    return train, validate, test\n",
    "\n",
    "def get_accuracy(truth, pred):\n",
    "    assert len(truth) == len(pred)\n",
    "    truth = torch.from_numpy(np.array(truth))\n",
    "    pred = torch.from_numpy(np.array(pred))\n",
    "    right = 0\n",
    "    for i in range(len(truth)):\n",
    "        if truth[i] == pred[i]:\n",
    "            right += 1.0\n",
    "    return right / len(truth)\n",
    "\n",
    "\n",
    "def train_epoch_progress(model, train_iter, loss_function, optimizer, text_field, label_field, epoch):\n",
    "    model.train()\n",
    "    avg_loss = 0.0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "    count = 0\n",
    "    for batch in tqdm(train_iter, desc='Train epoch '+str(epoch+1)):\n",
    "        sent, label = batch.review, batch.label\n",
    "        label.data.sub_(1)\n",
    "        truth_res += list(label.data)\n",
    "        model.batch_size = len(label.data)\n",
    "        model.hidden = model.init_hidden()\n",
    "        pred = model(sent)\n",
    "        pred_label = pred.data.max(1)[1].numpy()\n",
    "        pred_res += [x for x in pred_label]\n",
    "        model.zero_grad()\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data[0]\n",
    "        count += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss /= len(train_iter)\n",
    "    acc = get_accuracy(truth_res, pred_res)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def train_epoch(model, train_iter, loss_function, optimizer):\n",
    "    model.train()\n",
    "    avg_loss = 0.0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "    count = 0\n",
    "    for batch in train_iter:\n",
    "        sent, label = batch.review, batch.label\n",
    "        label.data.sub_(1)\n",
    "        truth_res += list(label.data)\n",
    "        model.batch_size = len(label.data)\n",
    "        model.hidden = model.init_hidden()\n",
    "        pred = model(sent)\n",
    "        pred_label = pred.data.max(1)[1].numpy()\n",
    "        pred_res += [x for x in pred_label]\n",
    "        model.zero_grad()\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data[0]\n",
    "        count += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss /= len(train_iter)\n",
    "    acc = get_accuracy(truth_res, pred_res)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def evaluate(model, data, loss_function, name):\n",
    "    model.eval()\n",
    "    avg_loss = 0.0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "    for batch in data:\n",
    "        sent, label = batch.review, batch.label\n",
    "        label.data.sub_(1)\n",
    "        truth_res += list(label.data)\n",
    "        model.batch_size = len(label.data)\n",
    "        model.hidden = model.init_hidden()\n",
    "        pred = model(sent)\n",
    "        pred_label = pred.data.max(1)[1].numpy()\n",
    "        pred_res += [x for x in pred_label]\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data[0]\n",
    "    avg_loss /= len(data)\n",
    "    acc = get_accuracy(truth_res,pred_res)\n",
    "    print(name + ': loss %.2f acc %.1f' % (avg_loss, acc*100))\n",
    "    return acc\n",
    "\n",
    "\n",
    "def load_sst(text_field, label_field, batch_size):\n",
    "    train,dev,test = train_validate_test_split(df)\n",
    "    train.to_csv('./train.csv')\n",
    "    dev.to_csv('./dev.csv')\n",
    "    test.to_csv('./test.csv')\n",
    "    train, dev, test = data.TabularDataset.splits(path='./', train='train.csv',\n",
    "                                                  validation='dev.csv', test='test.csv', format='csv',\n",
    "                                                  fields=[('review', text_field), ('label', label_field)])\n",
    "#     train,dev,test = train_validate_test_split(df)\n",
    "    text_field.build_vocab(train, dev, test)\n",
    "    label_field.build_vocab(train, dev, test)\n",
    "    train_iter, dev_iter, test_iter = data.BucketIterator.splits((train, dev, test),\n",
    "                batch_sizes=(batch_size, len(dev), len(test)), sort_key=lambda x: len(x.text), repeat=False, device=-1)\n",
    "    ## for GPU run\n",
    "#     train_iter, dev_iter, test_iter = data.BucketIterator.splits((train, dev, test),\n",
    "#                 batch_sizes=(batch_size, len(dev), len(test)), sort_key=lambda x: len(x.text), repeat=False, device=None)\n",
    "    return train_iter, dev_iter, test_iter\n",
    "\n",
    "\n",
    "# def adjust_learning_rate(learning_rate, optimizer, epoch):\n",
    "#     lr = learning_rate * (0.1 ** (epoch // 10))\n",
    "#     for param_group in optimizer.param_groups:\n",
    "#         param_group['lr'] = lr\n",
    "#     return optimizer\n",
    "\n",
    "\n",
    "# args = argparse.ArgumentParser()\n",
    "# args.add_argument('--m', dest='model', default='lstm', help='specify the mode to use (default: lstm)')\n",
    "# args = args.parse_args()\n",
    "\n",
    "EPOCHS = 20\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 150\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "timestamp = str(int(time.time()))\n",
    "best_dev_acc = 0.0\n",
    "\n",
    "\n",
    "text_field = data.Field(lower=True)\n",
    "label_field = data.Field(sequential=False)\n",
    "train_iter, dev_iter, test_iter = load_sst(text_field, label_field, BATCH_SIZE)\n",
    "\n",
    "model = LSTMSentiment(embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, vocab_size=len(text_field.vocab), label_size=len(label_field.vocab)-1,\\\n",
    "                          use_gpu=USE_GPU, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "if USE_GPU:\n",
    "    model = model.cuda()\n",
    "\n",
    "\n",
    "print('Load word embeddings...')\n",
    "# # glove\n",
    "# text_field.vocab.load_vectors('glove.6B.100d')\n",
    "\n",
    "# word2vector\n",
    "word_to_idx = text_field.vocab.stoi\n",
    "pretrained_embeddings = np.random.uniform(-0.25, 0.25, (len(text_field.vocab), 300))\n",
    "pretrained_embeddings[0] = 0\n",
    "word2vec = load_bin_vec('./GoogleNews-vectors-negative300.bin', word_to_idx)\n",
    "for word, vector in word2vec.items():\n",
    "    pretrained_embeddings[word_to_idx[word]-1] = vector\n",
    "\n",
    "# text_field.vocab.load_vectors(wv_type='', wv_dim=300)\n",
    "\n",
    "model.embeddings.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "# model.embeddings.weight.data = text_field.vocab.vectors\n",
    "# model.embeddings.embed.weight.requires_grad = False\n",
    "\n",
    "\n",
    "best_model = model\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "print('Training...')\n",
    "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "print(\"Writing to {}\\n\".format(out_dir))\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "for epoch in range(EPOCHS):\n",
    "    avg_loss, acc = train_epoch_progress(model, train_iter, loss_function, optimizer, text_field, label_field, epoch)\n",
    "    tqdm.write('Train: loss %.2f acc %.1f' % (avg_loss, acc*100))\n",
    "    dev_acc = evaluate(model, dev_iter, loss_function, 'Dev')\n",
    "    if dev_acc > best_dev_acc:\n",
    "        if best_dev_acc > 0:\n",
    "            os.system('rm '+ out_dir + '/best_model' + '.pth')\n",
    "        best_dev_acc = dev_acc\n",
    "        best_model = model\n",
    "        torch.save(best_model.state_dict(), out_dir + '/best_model' + '.pth')\n",
    "        # evaluate on test with the best dev performance model\n",
    "        test_acc = evaluate(best_model, test_iter, loss_function, 'Test')\n",
    "test_acc = evaluate(best_model, test_iter, loss_function, 'Final Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
