{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic Modules for data and text processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Keras Modules \n",
    "from keras.preprocessing.text import Tokenizer  # This tokenizes the text\n",
    "from keras.preprocessing.sequence import pad_sequences # This equalises the input we want to give\n",
    "from keras.models import Sequential # We will build sequential models only\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation # All of the layers of our model\n",
    "from keras.layers.embeddings import Embedding # How to build our word vectors\n",
    "\n",
    "# Plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('processed.csv',delimiter = \"\\t\",error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Unnamed: 0.1                                   To Subject  \\\n",
      "0           0             0  frozenset({'tim.belden@enron.com'})     NaN   \n",
      "\n",
      "                content     user  labeled  rep  \n",
      "0  Here is our forecast  allen-p    False    0  \n"
     ]
    }
   ],
   "source": [
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Clearning the Dataframe a bit\n",
    "df = df.drop(['Unnamed: 0','Unnamed: 0.1', 'Subject','user','labeled','To'], axis=1)\n",
    "df['label'] = df['rep']\n",
    "df = df.drop(['rep'],axis=1)\n",
    "#df = df[df.label != 'unsup']\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Normalising Function\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation \n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert Words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words (commonly used stuff eg, is and was)\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text) \n",
    "    \n",
    "    # Common Dictionary Corpus\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    ## Stemming\n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2)\n",
      "(99598, 2)\n"
     ]
    }
   ],
   "source": [
    "# Clear some text\n",
    "print(df.shape)\n",
    "\n",
    "# Drop empty rows (NaN)\n",
    "df = df.dropna()\n",
    "\n",
    "# Using the text cleaning function\n",
    "df['content'] = df['content'].map(lambda x: clean_text(x))\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Sequences\n",
    "vocabulary_size = 20000\n",
    "tokenizer = Tokenizer(num_words = vocabulary_size)\n",
    "tokenizer.fit_on_texts(df['content'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['content'])\n",
    "data = pad_sequences(sequences,maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached here\n",
      "X_train (74698, 50) \n",
      " y_train (74698,)\n"
     ]
    }
   ],
   "source": [
    "# Now splitting training and testing data\n",
    "\n",
    "print(\"Reached here\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, df['label'], test_size=.25)\n",
    "print(\"X_train {} \\n y_train {}\".format(X_train.shape,y_train.shape))\n",
    "#import csv\n",
    "#csv.field_size_limit()\n",
    "#csv.field_size_limit(10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Architecture begins here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 100)           2000000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 2,080,501\n",
      "Trainable params: 2,080,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Defining the Model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size,100,input_length=50))\n",
    "model.add(LSTM(100, dropout =0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 66698 samples, validate on 8000 samples\n",
      "Epoch 1/10\n",
      "66698/66698 [==============================] - 3s 42us/step - loss: 0.3032 - acc: 0.8621 - val_loss: 0.2889 - val_acc: 0.8741\n",
      "Epoch 2/10\n",
      "66698/66698 [==============================] - 3s 41us/step - loss: 0.2640 - acc: 0.8913 - val_loss: 0.2533 - val_acc: 0.8889\n",
      "Epoch 3/10\n",
      "66698/66698 [==============================] - 3s 41us/step - loss: 0.2244 - acc: 0.9112 - val_loss: 0.2280 - val_acc: 0.9074\n",
      "Epoch 4/10\n",
      "66698/66698 [==============================] - 3s 41us/step - loss: 0.1950 - acc: 0.9243 - val_loss: 0.2138 - val_acc: 0.9166\n",
      "Epoch 5/10\n",
      "66698/66698 [==============================] - 3s 41us/step - loss: 0.1731 - acc: 0.9343 - val_loss: 0.2073 - val_acc: 0.9202\n",
      "Epoch 6/10\n",
      "66698/66698 [==============================] - 3s 41us/step - loss: 0.1553 - acc: 0.9427 - val_loss: 0.2002 - val_acc: 0.9236\n",
      "Epoch 7/10\n",
      "66698/66698 [==============================] - 3s 41us/step - loss: 0.1411 - acc: 0.9483 - val_loss: 0.1939 - val_acc: 0.9273\n",
      "Epoch 8/10\n",
      "66698/66698 [==============================] - 3s 41us/step - loss: 0.1316 - acc: 0.9524 - val_loss: 0.1921 - val_acc: 0.9301\n",
      "Epoch 9/10\n",
      "66698/66698 [==============================] - 3s 41us/step - loss: 0.1219 - acc: 0.9563 - val_loss: 0.1873 - val_acc: 0.9314\n",
      "Epoch 10/10\n",
      "66698/66698 [==============================] - 3s 41us/step - loss: 0.1147 - acc: 0.9585 - val_loss: 0.1890 - val_acc: 0.9330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17a51187d68>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and Train the Model \n",
    "\n",
    "## HyperParameters\n",
    "batch_size = 8000\n",
    "num_epochs = 10\n",
    "\n",
    "# Making validation set\n",
    "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
    "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
    "\n",
    "model.fit(X_train2,y_train2, validation_data=(X_valid,y_valid), batch_size=batch_size,epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.9305622489959839\n"
     ]
    }
   ],
   "source": [
    "# Finding the accuracy\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test Accuracy: \",scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
