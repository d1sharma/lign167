{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('imdb_master.csv',encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0', 'type','file'], axis=1)\n",
    "df = df[df.label != 'unsup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49940</th>\n",
       "      <td>Oh yeah! Jenna Jameson did it again! Yeah Baby...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49941</th>\n",
       "      <td>This one is quite a nice surprise. Cute little...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49942</th>\n",
       "      <td>I watched this movie once and might watch it a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49943</th>\n",
       "      <td>Very nice action with an interwoven story whic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49944</th>\n",
       "      <td>If you've ever wondered why they don't make po...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49945</th>\n",
       "      <td>Before watching this movie I thought this movi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49946</th>\n",
       "      <td>this is really films outside (not in a motel r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49947</th>\n",
       "      <td>This is a more interesting than usual porn mov...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49948</th>\n",
       "      <td>Saw this movie twice at community screenings a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49949</th>\n",
       "      <td>Beforehand Notification: I'm sure someone is g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49950</th>\n",
       "      <td>This film is shockingly underrated on IMDb. Li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49951</th>\n",
       "      <td>I watch them all.&lt;br /&gt;&lt;br /&gt;It's not better t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49952</th>\n",
       "      <td>Well, if you are open-minded enough to have li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49953</th>\n",
       "      <td>This movie surprised me. Some things were \"cli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49954</th>\n",
       "      <td>I noticed this movie was getting trashed well ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49955</th>\n",
       "      <td>I must give How She Move a near-perfect rating...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49956</th>\n",
       "      <td>I was a bit scared to watch this movie due to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49957</th>\n",
       "      <td>I saw this movie when I was a little girl. And...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49958</th>\n",
       "      <td>Just get it. The DVD is cheap and easy to come...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49959</th>\n",
       "      <td>Perfect for families with small children who a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49960</th>\n",
       "      <td>I saw this movie when Mystery Science Theater ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49961</th>\n",
       "      <td>There are some things I will never understand;...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49962</th>\n",
       "      <td>My mother took me to see this film as a child ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49963</th>\n",
       "      <td>Shawshank, Godfather, Pulp Fiction... all good...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49964</th>\n",
       "      <td>This movie surprised me. Some things were \"cli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49965</th>\n",
       "      <td>Brilliant use of overstated technicolor illust...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49966</th>\n",
       "      <td>This film was Excellent, I thought that the or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49967</th>\n",
       "      <td>This is definitely an appropriate update for t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49968</th>\n",
       "      <td>I really liked this version of 'Vanishing Poin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49969</th>\n",
       "      <td>Sure this was a remake of a 70's film, but it ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49970</th>\n",
       "      <td>its not as good as the first movie,but its a g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49971</th>\n",
       "      <td>Sure, it was cheesy and nonsensical and at tim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49972</th>\n",
       "      <td>SPOILERS THROUGH: &lt;br /&gt;&lt;br /&gt;I really am in t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49973</th>\n",
       "      <td>I have to say, I loved Vanishing Point. I've s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49974</th>\n",
       "      <td>To start off with, since this movie is a remak...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49975</th>\n",
       "      <td>I have to agree with most of the other posts. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49976</th>\n",
       "      <td>Any movie that shows federal PIGs (Persons In ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49977</th>\n",
       "      <td>In Canadian director Kari Skogland's film adap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49978</th>\n",
       "      <td>I saw this movie last night after waiting ages...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49979</th>\n",
       "      <td>This movie is about basically human relations,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49980</th>\n",
       "      <td>I was surprised at just how much I enjoyed thi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49981</th>\n",
       "      <td>I saw this film in Winnipeg recently - appropr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49982</th>\n",
       "      <td>As perhaps one of the few Canadians who did no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49983</th>\n",
       "      <td>I was very moved by the story and because I am...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49984</th>\n",
       "      <td>What I loved about the on-screen adaptation of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49985</th>\n",
       "      <td>I had a chance to see a screening of this movi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49986</th>\n",
       "      <td>This is a really interesting movie. It is an a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49987</th>\n",
       "      <td>I saw the movie recently and really liked it. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49988</th>\n",
       "      <td>I thought this movie was hysterical. I have wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49989</th>\n",
       "      <td>...this is a classic with so many great dialog...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49990</th>\n",
       "      <td>The most hillarious and funny Brooks movie I e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49991</th>\n",
       "      <td>\"Life stinks\" is a parody of life and death, h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49992</th>\n",
       "      <td>This is the kind of film you want to see with ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49993</th>\n",
       "      <td>I have not read the other comments on the film...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49994</th>\n",
       "      <td>Life Stinks (1991) was a step below Mel Brooks...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>Seeing as the vote average was pretty low, and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The plot had some wretched, unbelievable twist...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am amazed at how this movie(and most others ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>A Christmas Together actually came before my t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>Working-class romantic drama from director Mar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  label\n",
       "49940  Oh yeah! Jenna Jameson did it again! Yeah Baby...      1\n",
       "49941  This one is quite a nice surprise. Cute little...      1\n",
       "49942  I watched this movie once and might watch it a...      1\n",
       "49943  Very nice action with an interwoven story whic...      1\n",
       "49944  If you've ever wondered why they don't make po...      1\n",
       "49945  Before watching this movie I thought this movi...      1\n",
       "49946  this is really films outside (not in a motel r...      1\n",
       "49947  This is a more interesting than usual porn mov...      1\n",
       "49948  Saw this movie twice at community screenings a...      1\n",
       "49949  Beforehand Notification: I'm sure someone is g...      1\n",
       "49950  This film is shockingly underrated on IMDb. Li...      1\n",
       "49951  I watch them all.<br /><br />It's not better t...      1\n",
       "49952  Well, if you are open-minded enough to have li...      1\n",
       "49953  This movie surprised me. Some things were \"cli...      1\n",
       "49954  I noticed this movie was getting trashed well ...      1\n",
       "49955  I must give How She Move a near-perfect rating...      1\n",
       "49956  I was a bit scared to watch this movie due to ...      1\n",
       "49957  I saw this movie when I was a little girl. And...      1\n",
       "49958  Just get it. The DVD is cheap and easy to come...      1\n",
       "49959  Perfect for families with small children who a...      1\n",
       "49960  I saw this movie when Mystery Science Theater ...      1\n",
       "49961  There are some things I will never understand;...      1\n",
       "49962  My mother took me to see this film as a child ...      1\n",
       "49963  Shawshank, Godfather, Pulp Fiction... all good...      1\n",
       "49964  This movie surprised me. Some things were \"cli...      1\n",
       "49965  Brilliant use of overstated technicolor illust...      1\n",
       "49966  This film was Excellent, I thought that the or...      1\n",
       "49967  This is definitely an appropriate update for t...      1\n",
       "49968  I really liked this version of 'Vanishing Poin...      1\n",
       "49969  Sure this was a remake of a 70's film, but it ...      1\n",
       "49970  its not as good as the first movie,but its a g...      1\n",
       "49971  Sure, it was cheesy and nonsensical and at tim...      1\n",
       "49972  SPOILERS THROUGH: <br /><br />I really am in t...      1\n",
       "49973  I have to say, I loved Vanishing Point. I've s...      1\n",
       "49974  To start off with, since this movie is a remak...      1\n",
       "49975  I have to agree with most of the other posts. ...      1\n",
       "49976  Any movie that shows federal PIGs (Persons In ...      1\n",
       "49977  In Canadian director Kari Skogland's film adap...      1\n",
       "49978  I saw this movie last night after waiting ages...      1\n",
       "49979  This movie is about basically human relations,...      1\n",
       "49980  I was surprised at just how much I enjoyed thi...      1\n",
       "49981  I saw this film in Winnipeg recently - appropr...      1\n",
       "49982  As perhaps one of the few Canadians who did no...      1\n",
       "49983  I was very moved by the story and because I am...      1\n",
       "49984  What I loved about the on-screen adaptation of...      1\n",
       "49985  I had a chance to see a screening of this movi...      1\n",
       "49986  This is a really interesting movie. It is an a...      1\n",
       "49987  I saw the movie recently and really liked it. ...      1\n",
       "49988  I thought this movie was hysterical. I have wa...      1\n",
       "49989  ...this is a classic with so many great dialog...      1\n",
       "49990  The most hillarious and funny Brooks movie I e...      1\n",
       "49991  \"Life stinks\" is a parody of life and death, h...      1\n",
       "49992  This is the kind of film you want to see with ...      1\n",
       "49993  I have not read the other comments on the film...      1\n",
       "49994  Life Stinks (1991) was a step below Mel Brooks...      1\n",
       "49995  Seeing as the vote average was pretty low, and...      1\n",
       "49996  The plot had some wretched, unbelievable twist...      1\n",
       "49997  I am amazed at how this movie(and most others ...      1\n",
       "49998  A Christmas Together actually came before my t...      1\n",
       "49999  Working-class romantic drama from director Mar...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df['label'].str.lower().replace({'pos': 1, 'neg': 0})\n",
    "# df.tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time, random\n",
    "from tqdm import tqdm\n",
    "from torchtext import data\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is an example of why the majority of action films are the same. Generic and boring, there's really nothing worth watching here. A complete waste of the then barely-tapped talents of Ice-T and Ice Cube, who've each proven many times over that they are capable of acting, and acting well. Don't bother with this one, go see New Jack City, Ricochet or watch New York Undercover for Ice-T, or Boyz n the Hood, Higher Learning or Friday for Ice Cube and see the real deal. Ice-T's horribly cliched dialogue alone makes this film grate at the teeth, and I'm still wondering what the heck Bill Paxton was doing in this film? And why the heck does he always play the exact same character? From Aliens onward, every film I've seen with Bill Paxton has him playing the exact same irritating character, and at least in Aliens his character died, which made it somewhat gratifying...<br /><br />Overall, this is second-rate action trash. There are countless better films to see, and if you really want to see this one, watch Judgement Night, which is practically a carbon copy but has better acting and a better script. The only thing that made this at all worth watching was a decent hand on the camera - the cinematography was almost refreshing, which comes close to making up for the horrible film itself - but not quite. 4/10.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ribhu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "def cleanup(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join(ch for ch in text if ch not in exclude)\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    text = text.split()\n",
    "    texts = []\n",
    "    for word in text:\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        texts.append(word)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for sent in df.review:\n",
    "    corpus.append(cleanup(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for label in df.label:\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "wordvec = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.76074219e-02,  8.15429688e-02,  4.56542969e-02,  9.17968750e-02,\n",
       "       -1.47094727e-02,  1.11328125e-01,  6.54296875e-02, -9.66796875e-02,\n",
       "        1.38671875e-01,  1.43554688e-01, -7.86132812e-02, -1.63085938e-01,\n",
       "       -9.17968750e-02, -7.12890625e-02, -8.05664062e-02,  8.83789062e-02,\n",
       "        2.17773438e-01,  9.96093750e-02,  1.19018555e-02, -1.35742188e-01,\n",
       "       -5.78613281e-02,  9.03320312e-03,  1.75781250e-01, -1.17187500e-02,\n",
       "        4.90722656e-02, -1.05468750e-01, -1.53320312e-01,  6.17675781e-02,\n",
       "        8.64257812e-02,  2.25830078e-02,  1.59740448e-05,  6.39648438e-02,\n",
       "        2.33154297e-02, -1.45874023e-02,  1.51367188e-02,  5.29785156e-02,\n",
       "       -1.34277344e-02,  7.99560547e-03,  2.63671875e-02,  2.99072266e-02,\n",
       "        1.61132812e-01,  5.68847656e-02,  1.18164062e-01, -3.12500000e-02,\n",
       "       -7.59887695e-03,  6.64062500e-02, -1.08032227e-02, -6.53076172e-03,\n",
       "        1.13281250e-01,  9.22851562e-02,  4.29687500e-02, -6.10351562e-03,\n",
       "        2.80761719e-02, -8.78906250e-02,  3.46679688e-02,  8.83789062e-02,\n",
       "        8.36181641e-03,  1.30004883e-02,  3.71093750e-02, -5.12695312e-02,\n",
       "       -8.60595703e-03,  4.78515625e-02, -1.29882812e-01, -1.61132812e-01,\n",
       "       -8.10546875e-02, -9.70458984e-03,  5.12695312e-02,  9.61914062e-02,\n",
       "       -7.12890625e-02,  5.63964844e-02, -7.87353516e-03,  1.38671875e-01,\n",
       "       -2.52685547e-02,  7.22656250e-02, -1.61132812e-01, -2.91748047e-02,\n",
       "        1.05468750e-01,  2.27050781e-02,  6.29882812e-02,  4.18090820e-03,\n",
       "        1.28173828e-02,  4.07714844e-02,  3.66210938e-02,  6.88476562e-02,\n",
       "       -8.20312500e-02, -1.16210938e-01, -9.91210938e-02,  1.65039062e-01,\n",
       "       -1.34765625e-01, -5.63964844e-02,  2.73437500e-02,  4.46777344e-02,\n",
       "       -1.51977539e-02, -9.47265625e-02,  4.00390625e-02, -9.52148438e-02,\n",
       "       -6.15234375e-02,  1.67236328e-02, -1.62109375e-01,  4.61425781e-02,\n",
       "       -7.61718750e-02, -3.19824219e-02,  1.04980469e-01,  1.12304688e-02,\n",
       "        4.61425781e-02, -9.08203125e-02, -5.17578125e-02, -1.05468750e-01,\n",
       "        6.29882812e-02, -6.49414062e-02,  2.24609375e-02,  6.12792969e-02,\n",
       "        1.55029297e-02,  3.29589844e-02,  6.98242188e-02, -2.16064453e-02,\n",
       "        9.46044922e-03,  5.56640625e-02,  2.31933594e-02,  1.25000000e-01,\n",
       "       -1.96289062e-01,  1.25976562e-01, -1.55639648e-02,  8.88671875e-02,\n",
       "       -9.08203125e-02, -6.46972656e-03,  2.02178955e-04, -7.17773438e-02,\n",
       "       -6.54296875e-02, -8.10546875e-02,  3.63769531e-02, -1.57226562e-01,\n",
       "        1.57470703e-02, -6.22558594e-02, -5.78613281e-02, -1.39648438e-01,\n",
       "       -7.42187500e-02, -1.09252930e-02, -4.66308594e-02, -5.12695312e-02,\n",
       "       -9.99450684e-04, -7.17773438e-02,  3.36914062e-02, -1.63574219e-02,\n",
       "        4.58984375e-02,  5.27343750e-02, -1.04492188e-01, -1.41601562e-01,\n",
       "       -2.09960938e-01,  1.16210938e-01, -2.44140625e-02,  6.20117188e-02,\n",
       "       -2.25830078e-02,  2.69775391e-02, -1.14135742e-02, -4.24804688e-02,\n",
       "       -2.02941895e-03, -5.56640625e-02, -4.41894531e-02,  4.10156250e-02,\n",
       "        9.13085938e-02,  8.25195312e-02,  8.69140625e-02,  7.95898438e-02,\n",
       "        2.91748047e-02, -9.03320312e-02, -7.22656250e-02,  1.03515625e-01,\n",
       "       -2.96630859e-02, -6.44531250e-02, -1.75781250e-01,  2.66113281e-02,\n",
       "       -6.64062500e-02, -1.42211914e-02, -6.54296875e-02,  3.97949219e-02,\n",
       "        3.05175781e-03, -1.20605469e-01,  1.96533203e-02, -7.42187500e-02,\n",
       "       -5.00488281e-02, -9.96093750e-02,  4.58984375e-02,  1.65039062e-01,\n",
       "        2.16674805e-03, -3.75976562e-02, -7.61718750e-02,  2.73437500e-02,\n",
       "        1.87988281e-02,  6.89697266e-03,  1.68457031e-02, -9.22851562e-02,\n",
       "       -1.38549805e-02, -5.02929688e-02, -9.71679688e-02, -3.06396484e-02,\n",
       "       -5.39550781e-02, -3.08837891e-02, -7.76367188e-02, -1.95312500e-01,\n",
       "        6.15234375e-02, -1.04980469e-01, -1.80664062e-02,  4.54101562e-02,\n",
       "        6.73828125e-02, -8.64257812e-02, -9.86328125e-02, -2.84423828e-02,\n",
       "        2.70996094e-02, -2.88085938e-02, -1.84326172e-02,  6.64062500e-02,\n",
       "       -3.41796875e-02,  9.86328125e-02, -1.47460938e-01,  1.87988281e-02,\n",
       "        1.36718750e-01, -1.23291016e-02, -5.88378906e-02,  1.29394531e-02,\n",
       "        8.93554688e-02,  7.22656250e-02,  7.03125000e-02, -6.29882812e-02,\n",
       "        1.54296875e-01, -4.58984375e-02,  1.11816406e-01, -4.85839844e-02,\n",
       "        9.27734375e-02,  5.12695312e-02,  3.24707031e-02, -3.29589844e-02,\n",
       "       -2.41699219e-02,  6.10351562e-03,  1.00585938e-01,  2.86865234e-03,\n",
       "       -1.07421875e-01, -4.85839844e-02,  1.20605469e-01, -1.86920166e-03,\n",
       "        8.54492188e-02, -4.12597656e-02,  3.02734375e-02, -5.61523438e-02,\n",
       "        4.85839844e-02, -3.22265625e-02,  2.27050781e-02, -1.85546875e-02,\n",
       "        1.37939453e-02,  7.42187500e-02,  8.88671875e-02,  4.66308594e-02,\n",
       "        1.27929688e-01,  8.00781250e-02,  1.09375000e-01, -1.51367188e-02,\n",
       "        6.39648438e-02,  1.07910156e-01, -7.91015625e-02, -4.05273438e-02,\n",
       "       -5.46875000e-02,  4.02832031e-02, -1.39648438e-01,  9.71679688e-02,\n",
       "       -2.58789062e-02,  1.03515625e-01, -2.89306641e-02, -1.43432617e-02,\n",
       "        4.45556641e-03, -1.48315430e-02,  1.64062500e-01,  1.86767578e-02,\n",
       "        1.75781250e-01, -1.51977539e-02, -5.22460938e-02, -9.42382812e-02,\n",
       "        7.71484375e-02, -2.02148438e-01, -5.78613281e-02, -2.47802734e-02,\n",
       "       -5.21850586e-03, -2.91748047e-02,  1.08886719e-01,  7.27539062e-02,\n",
       "        8.54492188e-02, -8.88671875e-02, -8.77380371e-05, -1.40625000e-01,\n",
       "       -6.34765625e-02,  1.07910156e-01, -1.14746094e-01,  4.15039062e-02,\n",
       "       -4.19921875e-02,  9.22851562e-02, -7.13348389e-04,  7.51953125e-02,\n",
       "        4.93164062e-02, -5.56640625e-02,  1.04980469e-01, -1.08398438e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec.get_vector('but')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting corpus of sentences into train, test, validation\n",
    "sen_vec = []\n",
    "X_train = []\n",
    "X_cv = []\n",
    "X_test = []\n",
    "corp_train = corpus[0:200]\n",
    "corp_dev = corpus[200:300]\n",
    "corp_test = corpus[300:400]\n",
    "for sentence in corp_train:\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            sen_vec.append(wordvec.vocab[word].index)\n",
    "        except KeyError:\n",
    "            sen_vec.append(0) #If word not in vocab add tensor of 1s so that it doesnt affect the result\n",
    "    input_vec = torch.LongTensor(sen_vec)\n",
    "    X_train.append(input_vec)\n",
    "    sen_vec = []\n",
    "for sentence in corp_dev:\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            sen_vec.append(wordvec.vocab[word].index)\n",
    "        except KeyError:\n",
    "            sen_vec.append(0) #If word not in vocab add tensor of 1s so that it doesnt affect the result\n",
    "    input_vec = torch.LongTensor(sen_vec)\n",
    "    X_cv.append(input_vec)\n",
    "    sen_vec = []\n",
    "for sentence in corp_test:\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            sen_vec.append(wordvec.vocab[word].index)\n",
    "        except KeyError:\n",
    "            sen_vec.append(0) #If word not in vocab add tensor of 1s so that it doesnt affect the result\n",
    "    input_vec = torch.LongTensor(sen_vec)\n",
    "    X_test.append(input_vec)\n",
    "    sen_vec = []\n",
    "                                    #Should try with randoms as a variation if shit don't work\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPlitt\n",
    "labels_train = labels[0:200]\n",
    "labels_cv = labels[200:300]\n",
    "labels_test = labels[300:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "#         self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(wordvec.vectors))\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "                torch.zeros(1, 1, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        y = self.hidden2tag(lstm_out[-1])\n",
    "        log_probs = F.log_softmax(y,dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDING_DIM = 300\n",
    "# HIDDEN_DIM = 150\n",
    "# EPOCHS = 20\n",
    "# avg_loss = 0.0\n",
    "# Hard_num = 200\n",
    "\n",
    "# parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "# model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, 300000, 2)\n",
    "# loss_function = nn.NLLLoss()\n",
    "# optimizer = optim.Adam(parameters,lr = 1e-3)\n",
    "# for epoch in range(Hard_num):\n",
    "#     model.zero_grad()\n",
    "#     model.hidden = model.init_hidden()\n",
    "#     sentence_in = inputs[epoch]\n",
    "#     target = torch.tensor(labels[epoch], dtype=torch.long)\n",
    "#     tag_scores = model(sentence_in)\n",
    "#     loss = loss_function(tag_scores, target.unsqueeze_(0))\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "# avg_loss /= Hard_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    EMBEDDING_DIM = 300\n",
    "    HIDDEN_DIM = 150\n",
    "    EPOCH = 20\n",
    "    best_dev_acc = 0.0\n",
    "    train_size = len(labels_train)\n",
    "    dev_size = len(labels_cv)\n",
    "    test_size= len(labels_test)\n",
    "    model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, 300000, 2)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(parameters,lr = 1e-3)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr = 1e-2)\n",
    "    no_up = 0\n",
    "    for i in range(EPOCH):\n",
    "        print('epoch: %d start!' % i)\n",
    "        train_epoch(model, train_size, loss_function, optimizer, X_train, labels_train, i)\n",
    "        print('now best dev acc:',best_dev_acc)\n",
    "        dev_acc = evaluate(model,dev_size,loss_function,X_cv, labels_cv,'dev')\n",
    "        test_acc = evaluate(model, test_size, loss_function, X_test, labels_test, 'test')\n",
    "        if dev_acc > best_dev_acc:\n",
    "            best_dev_acc = dev_acc\n",
    "            os.system('rm mr_best_model_acc_*.model')\n",
    "            print('New Best Dev!!!')\n",
    "#             torch.save(model.state_dict(), 'best_models/mr_best_model_acc_' + str(int(test_acc*10000)) + '.model')\n",
    "            no_up = 0\n",
    "        else:\n",
    "            no_up += 1\n",
    "            if no_up >= 10:\n",
    "                exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(truth, pred):\n",
    "    assert len(truth)==len(pred)\n",
    "    right = 0\n",
    "    for i in range(len(truth)):\n",
    "        if truth[i]==pred[i]:\n",
    "            right += 1.0\n",
    "    return right/len(truth)\n",
    "\n",
    "def train_epoch(model, datapoints, loss_function, optimizer, text, labels, i):\n",
    "    model.train()\n",
    "    \n",
    "    avg_loss = 0.0\n",
    "    count = 0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "    batch_sent = []\n",
    "    \n",
    "    truth_res = labels\n",
    "\n",
    "    for j in range(datapoints):\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "        sent = text[j]\n",
    "        label = torch.tensor(labels[j], dtype=torch.long)\n",
    "        pred = model(sent)\n",
    "        pred_label = pred.data.max(1)[1].numpy()\n",
    "        pred_res.append(pred_label)\n",
    "        model.zero_grad()\n",
    "        loss = loss_function(pred, label.unsqueeze_(0))\n",
    "        avg_loss += loss.data[0]\n",
    "        count += 1\n",
    "        if count % 500 == 0:\n",
    "            print('epoch: %d iterations: %d loss :%g' % (i, count, loss.data[0]))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss /= datapoints\n",
    "    print('epoch: %d done! \\n train avg_loss:%g , acc:%g'%(i, avg_loss, get_accuracy(truth_res,pred_res)))\n",
    "\n",
    "def evaluate(model, datapoints, loss_function, text, labels, name ='dev'):\n",
    "    model.eval()\n",
    "    avg_loss = 0.0\n",
    "    truth_res = labels\n",
    "    pred_res = []\n",
    "\n",
    "    for j in range(datapoints):\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "        sent = text[j]\n",
    "        label = torch.tensor(labels[j], dtype=torch.long)\n",
    "        pred = model(sent)\n",
    "        pred_label = pred.data.max(1)[1].numpy()\n",
    "        pred_res.append(pred_label)\n",
    "        # model.zero_grad() # should I keep this when I am evaluating the model?\n",
    "        loss = loss_function(pred, label.unsqueeze_(0))\n",
    "        avg_loss += loss.data[0]\n",
    "    avg_loss /= datapoints\n",
    "    acc = get_accuracy(truth_res, pred_res)\n",
    "    print(name + ' avg_loss:%g train acc:%g' % (avg_loss, acc ))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 start!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (2, 1, 150), got (1, 1, 150)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-b918a6aff303>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch: %d start!'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'now best dev acc:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_dev_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdev_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_cv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-d7cfc51e0ac9>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, datapoints, loss_function, optimizer, text, labels, i)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mpred_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-7d00eb8c08f6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         lstm_out, self.hidden = self.lstm(\n\u001b[0;32m---> 28\u001b[0;31m             embeds.view(len(sentence), 1, -1), self.hidden)\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mflat_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         func = self._backend.RNN(\n\u001b[1;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             check_hidden_size(hidden[0], expected_hidden_size,\n\u001b[0;32m--> 147\u001b[0;31m                               'Expected hidden[0] size {}, got {}')\n\u001b[0m\u001b[1;32m    148\u001b[0m             check_hidden_size(hidden[1], expected_hidden_size,\n\u001b[1;32m    149\u001b[0m                               'Expected hidden[1] size {}, got {}')\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcheck_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Expected hidden size {}, got {}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (2, 1, 150), got (1, 1, 150)"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
